{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidhishah9/News-Articles-Classification/blob/main/ece219Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWF2yBKHpw7U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRNmAOAiqZk-",
        "outputId": "e402a695-a299-4e09-8757-0a7d85bae43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['full_text', 'summary', 'keywords', 'publish_date', 'authors', 'url', 'leaf_label', 'root_label']\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('Project1-ClassificationDataset.csv')\n",
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UiwKRnDraLE",
        "outputId": "be9e7347-3842-4a6c-8bf5-921666c030a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3476\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "count_row = df.shape[0]\n",
        "count_col = df.shape[1]\n",
        "print(count_row)\n",
        "print(count_col)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "Cawi00wGx6l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df[[\"full_text\",\"root_label\"]], test_size=0.2)"
      ],
      "metadata": {
        "id": "J2EkJcxi1P1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean(text):\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  texter = re.sub(r\"<br />\", \" \", text)\n",
        "  texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
        "  texter = re.sub('&#39;', \"\\\"\", texter)\n",
        "  texter = re.sub('\\n', \" \", texter)\n",
        "  texter = re.sub(' u ',\" you \", texter)\n",
        "  texter = re.sub('`',\"\", texter)\n",
        "  texter = re.sub(' +', ' ', texter)\n",
        "  texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
        "  texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
        "  texter = re.sub('&amp;', 'and', texter)\n",
        "  texter = re.sub('\\r', ' ',texter)\n",
        "  clean = re.compile('<.*?>')\n",
        "  texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
        "  texter = re.sub(clean, '', texter)\n",
        "  if texter == \"\":\n",
        "    texter = \"\"\n",
        "  return texter"
      ],
      "metadata": {
        "id": "die52I6n1WMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.map(clean)\n",
        "test = test.map(clean)\n",
        "\n"
      ],
      "metadata": {
        "id": "tjsnin9I4LyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "#CountVectorizer - Converts text into a Bag-of-Words representation (word frequency counts).\n",
        "\n",
        "#TfidTransformer - Converts word counts into TF-IDF scores (weights words based on importance)\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "wnl = nltk.wordnet.WordNetLemmatizer()  #Lemmmatizing - reduces words into it's base form\n",
        "\n",
        "analyzer = CountVectorizer().build_analyzer() #CountVectorizer - splits text into words, .build_analyzer is used to create an analyzer to do stuff with the text like lower case, remove punctuation, etc\n",
        "\n",
        "#Below functions are all text processing functoins used before vectorization\n",
        "\n",
        "#POS tagging in nltk.pos_tag() follows the Penn Treebank format.\n",
        "#WordNetLemmatizer needs a different format (it expects n, v, a, r).\n",
        "#This function maps Penn POS tags → WordNet tags.\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "\n",
        "\n",
        "#Below functions\n",
        "#Receives a list of words.\n",
        "#Finds their part-of-speech (POS) tags.\n",
        "#Lemmatizes them using the correct POS tags.\n",
        "\n",
        "def lemmatize_sent(list_word):\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
        "            for word, tag in pos_tag(list_word)]\n",
        "\n",
        "#Uses analyzer(doc) to split the text into words.\n",
        "#Lemmatizes each word.\n",
        "#Removes numbers (if not word.isdigit()).\n",
        "\n",
        "def rmv_nums(doc):\n",
        "    return (word for word in lemmatize_sent(analyzer(doc))\n",
        "            if not word.isdigit())\n",
        "\n"
      ],
      "metadata": {
        "id": "pFVIs48aV77h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVH-FJDd9HKm",
        "outputId": "d5a27796-4fcf-4120-a751-1c3f6ae542e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer = rmv_nums, min_df = 3, stop_words = 'english')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Fits CountVectorizer on the training data:\n",
        "#Extracts unique words (vocabulary) from train['full_text'].\n",
        "#Converts text into a word count matrix.\n",
        "\n",
        "#Step 1 and 2\n",
        "# The fit() method starts processing each document in train['full_text'].\n",
        "# For each document, CountVectorizer calls: rmv_nums(document)\n",
        "# This function processes the raw text, e.g., removing numbers.\n",
        "# It returns a list of words (tokens) for that document.\n",
        "\n",
        "\n",
        "# Step 3: Building the Vocabulary\n",
        "# After all documents are processed by rmv_nums, CountVectorizer:\n",
        "  # Counts all words in the dataset.\n",
        "  # Removes stopwords (if stop_words='english').\n",
        "  # Removes words appearing in fewer than 3 documents (min_df=3).\n",
        "  # Creates a dictionary (vocabulary) mapping words to unique indices.\n",
        "\n",
        "# Step 4: vectorizer.transform() Converts Text into a Sparse Matrix\n",
        "  # Now, transform() processes the original documents again:\n",
        "    # Uses the learned vocabulary to create a word count matrix.\n",
        "    # Each document becomes a row, where:\n",
        "    # Columns correspond to words in the vocabulary.\n",
        "    # Each cell contains the count of a word in that document.\n",
        "    # This results in a sparse matrix (since most words don’t appear in every document).\n",
        "\n",
        "X_train_counts = vectorizer.fit_transform(train['full_text'])\n",
        "print(X_train_counts)\n",
        "X_test_counts = vectorizer.transform(test['full_text'])\n",
        "\n",
        "# TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Fit and transform training data using TfidfTransformer\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "# Transform testing data using fitted TfidfTransformer\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Print Results\n",
        "print(\"Shape of TF-IDF-processed train matrix:\", X_train_tfidf.shape)\n",
        "print(\"Shape of TF-IDF-processed test matrix:\", X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w_7irqh8hwv",
        "outputId": "1e593c7d-a027-40b5-90af-a6b98823274c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 13270)\t2\n",
            "  (0, 12103)\t16\n",
            "  (0, 4788)\t2\n",
            "  (0, 3076)\t8\n",
            "  (0, 365)\t4\n",
            "  (0, 3809)\t3\n",
            "  (0, 5952)\t3\n",
            "  (0, 1157)\t13\n",
            "  (0, 7753)\t4\n",
            "  (0, 12538)\t5\n",
            "  (0, 5155)\t5\n",
            "  (0, 4520)\t5\n",
            "  (0, 10827)\t2\n",
            "  (0, 12252)\t12\n",
            "  (0, 11933)\t3\n",
            "  (0, 5987)\t7\n",
            "  (0, 571)\t2\n",
            "  (0, 4153)\t2\n",
            "  (0, 4231)\t1\n",
            "  (0, 12985)\t2\n",
            "  (0, 12246)\t1\n",
            "  (0, 13322)\t3\n",
            "  (0, 5143)\t2\n",
            "  (0, 5609)\t1\n",
            "  (0, 4725)\t2\n",
            "  :\t:\n",
            "  (2779, 11862)\t1\n",
            "  (2779, 3825)\t1\n",
            "  (2779, 9941)\t2\n",
            "  (2779, 9313)\t1\n",
            "  (2779, 1108)\t2\n",
            "  (2779, 8108)\t1\n",
            "  (2779, 6828)\t1\n",
            "  (2779, 2336)\t1\n",
            "  (2779, 5141)\t1\n",
            "  (2779, 4077)\t1\n",
            "  (2779, 12320)\t2\n",
            "  (2779, 6693)\t1\n",
            "  (2779, 3054)\t1\n",
            "  (2779, 10435)\t1\n",
            "  (2779, 8129)\t1\n",
            "  (2779, 13474)\t2\n",
            "  (2779, 1787)\t2\n",
            "  (2779, 6756)\t1\n",
            "  (2779, 7215)\t1\n",
            "  (2779, 6871)\t1\n",
            "  (2779, 7426)\t1\n",
            "  (2779, 717)\t1\n",
            "  (2779, 4101)\t1\n",
            "  (2779, 1925)\t1\n",
            "  (2779, 10637)\t1\n",
            "Shape of TF-IDF-processed train matrix: (2780, 13594)\n",
            "Shape of TF-IDF-processed test matrix: (696, 13594)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "model = NMF(n_components=50, init='random', random_state=0)\n",
        "W_train = model.fit_transform(X_train_tfidf)\n",
        "\n",
        "print(W_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnuyNocvsjPw",
        "outputId": "a306ea39-73f0-4116-8685-3fc64ca952ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2780, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/hws7k8BjAyaGcrWZss59",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}